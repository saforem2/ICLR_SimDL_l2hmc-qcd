%
\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{xcolor}
% \usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,factor=1100,stretch=10,shrink=10]{microtype}
\setlength{\marginparwidth}{2cm}
% \usepackage[colorinlistoftodos]{todonotes}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{subcaption}
\usepackage[ruled, vlined,linesnumbered,inoutnumbered,rightnl]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows}
\newcommand{\JCO}[1]{\textcolor{red}{[#1]}}

\usepackage[unicode,linktocpage=true,breaklinks]{hyperref}% add hypertext capabilities
\hypersetup{
  colorlinks=true,
  urlcolor=green,
  linkcolor=magenta,
  citecolor=cyan,
}

\definecolor{blue1}{HTML}{448aff}
\definecolor{pink1}{HTML}{FA5477}

\newcommand{\mbart}{\textcolor{red}{\bar{m}^{t}}}
\newcommand{\mt}{\textcolor{blue}{m^{t}}}

% \RequirePackage{lineno}
% \linenumbers\relax % Commence numbering lines

\title{Neural Transformations for \\Efficient Topological Mixing}% in Lattice Gauge Theory}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Sam Foreman, Xiao-Yong Jin\& James Osborn\thanks{\hyperref{%
      https://github.com/saforem2/l2hmc-qcd
   }{https://github.com/saforem2/l2hmc-qcd} \\
   Leadership Computing Facility\\
   Argonne National Laboratory\\
   Lemont, IL 60439
   \texttt{\{foremans,xjin,\}@anl.gov},%
   \texttt{\{osborn\}@alcf.anl.gov}\\
}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
   We propose a generalized version of the L2HMC algorithm~\citep{levy2017}, and evaluate its ability to sample from
   different topologies in a two-dimensional lattice gauge theory.
   %
   In particular, we demonstrate that our model is able to successfully mix between modes of different topology,
   significantly reducing the computational cost required to generate independent gauge configurations.
\end{abstract}

% \section{\label{sec:introduction}Introduction}
% \color{red}{TODO:\@ Complete introduction}\color{black}
\section{TODO}
\begin{enumerate}
   \item Instead of using a time step to vary the network inputs, we use completely different networks for each leapfrog
      step in our trajectory.
   \item Explain that ``instead of using \(\|x^{\prime}-x\|\), in \(\mathcal{L}_{\theta}\) we use \ldots''
   \item Remove ``During training we maintain a buffer of \(M=2048\)\ldots'' from \Secref{sec:main_contributions}
   \item Combine \((5.)\), \((6.)\), \((7.)\) in \Secref{sec:main_contributions}
   \item Remove \Eqref{eq:wilsonaction}, combine with \Eqref{eq:annealingdistribution} and move inline
   \item Explain \(\mathcal{Q}_{\mathbb{Z}}\) is a physical quantity (winding number), explain we want to maximize
      difference but need continuous variable for training, hence \(\mathcal{Q}_{\mathbb{R}}\)
   \item Change notation for lattice gauge theory, \(\varphi_{\mu}(x)\rightarrow x_{\mu}(n)\) to be consistent with
      \Secref{sec:l2hmc}.
\end{enumerate}
%
\section{\label{sec:main_contributions}Main Contributions}
\begin{enumerate}
   \item We propose a generalized version of the L2HMC algorithm that uses different networks (with different step
      sizes) for each distinct leapfrog step.
      %
      We represent this generalization by carrying a discrete index \(k = 0, 1, \ldots, N_{\mathrm{LF}}\) through the
      augmented leapfrog equations, (\Eqref{eq:new_momentum_update}, \Eqref{eq:new_position_update}) indicating that
      these functions are allowed to vary.
      %
   \item We propose a modified loss function, defined in terms of the topological charge metric
      \(\delta_{\mathcal{Q}_{\mathbb{R}}}(\xi^{\prime}, \xi)\),
      %
      \begin{equation}
         \mathcal{L}_{\theta}{\left[\xi^{\prime},\xi,A(\xi^{\prime}|\xi)\right]} =%
         \frac{-A(\xi^{\prime}|\xi)\cdot \delta_{\mathcal{Q}_{\mathbb{R}}}(\xi^{\prime},\xi)}{a^{2}},
      \end{equation}
      %
      where \(\delta_{\mathcal{Q}_{\mathbb{R}}}(\xi^{\prime}, \xi) \equiv
      {\left(\mathcal{Q}_{\mathbb{R}}^{\prime}-\mathcal{Q}_{\mathbb{R}}\right)}^{2}\), and
      \(\mathcal{Q}_{\mathbb{R}}\in\mathbb{R}\) is the \emph{real-valued topological charge}, defined in
      \Secref{sec:lattice_gauge_theory}.
      %
      We evaluate this loss at the end of each trajectory during training, and use the gradient information to update
      the weights \(\theta\) parameterizing the auxiliary functions \(s^{k}_{i}, t^{k}_{i}, q^{k}_{i}\), \(i = x, v\)
      introduced in the augmented leapfrog updates in \Secref{sec:l2hmc}.
      %
   \item During training we maintain a buffer of \(M = 2048\) chains (``batch size'') which are updated in parallel.
      %
      Expectation values of physical quantities are calculated as averages over these \(M\) chains (after dropping the
      first \(\sim25\%\) of configurations for thermalization), and we use blocked jackknife resampling to compute
      the error estimates.
      %
   \item We introduce an \emph{annealing schedule} (\Secref{sec:annealing_schedule}) \({\{\gamma\}}_{t=0}^{N} =
      \{\gamma_{0}, \gamma_{1}, \ldots \gamma_{N}\}\), with \(\gamma_{0} < \gamma_{1} < \cdots < \gamma_{N}\), with
      \(|\gamma_{t}|<1\) and \(\gamma_{t+1} - \gamma_{t} \ll 1\) that is slowly varied during training (and removed
      entirely during inference) that scales the potential energy function in our target distribution, \(p_{t}(x)\propto
      e^{\gamma_{t}S(x)}\), that allows our sampler to explore previously inaccessible regions of the phase space by
      shrinking the height of the various energy barriers separating isolated modes.
      %
   \item We apply the proposed method to a \(1+1\)-dimensional \(U(1)\) lattice gauge theory defined on a
      \(N_{x}\times N_{t}\) lattice with periodic boundary conditions, and report a significant reduction in the
      computational cost required to generate independent \emph{gauge configurations}.
      %
   \item We use the \emph{integrated autocorrelation time} of the topological charge
      \(\tau_{\mathrm{int}}^{\mathcal{Q}}\) as a metric for determining the efficiency of our trained sampler.
      %
      The improvement for the trained model compared to generic HMC can be seen in \Figref{fig:autocorr_vs_beta}.
      %
   \item We compare our results to traditional HMC across a variety of trajectory lengths and inverse coupling
      constants \(\beta\), and show that our trained model consistently outperforms traditional HMC.\@
      %
      These results can be seen in \Secref{subsec:autocorr_plots}.
\end{enumerate}
%
\section{\label{sec:related_work}Related Work:}
Recently, there has been a significant interest in applying probabilistic programming techniques with
autodifferentiation capabilities to develop more efficient simulations.
\JCO{not sure what this means: probabilistic programming techniques}
%
% Due to the widespread applicability of these techniques, as well as their central importance to virtually all scientific
% disciplines,
% scientific disciplines, there exist significant opportunities for interdisciplinary researc
% , there are ample opportunities for interdisciplinary research
% groups with cross-cutting applications.

% Because the use of Bayesian inference/generative modeling is of great importance across virtually all scientific
% disciplines,
% optimized simulations for a variety of scientific disciplines.
%
In particular, following the development of the RealNVP \citep{dinhRealNVP} architecture, there has been an explosion of
different proposals that aim to take advantage of the invertible network architecture.
%
Because of the enormous computational burden faced by generative techniques in lattice gauge theory, there have been
multiple works
\citep{%
   albergo2019flow,albergo2021introduction,favoni2020lattice,medvidovic2020generative,neklyudov2020orbital,
   neklyudov2020involutive, li2020neural,boyda2020sampling,kanwar2020equivariant,toth2019hamiltonian,
   hoffman2019neutra,wehenkel2020you,pasarica2010adaptively, dinhRealNVP,tanaka2017towards,schaefer2009investigating,
   cossu2018testing,rezende2020normalizing%
}, 
that look at developing more efficient sampling techniques specifically tailored for lattice gauge/field theories.
% timprove existing sampling techniques by introducing some trainable architecture that encourages various
% properties.
%
% Of particular interest and relevance to our work, we
% proposed methods that take advantage of the invertible network architecture to construct various models (e.g. VAE, GAN,
% RBM,
% etc.)
% Related work:
% \citep{albergo2021introduction,favoni2020lattice,medvidovic2020generative,neklyudov2020orbital,li2020neural,boyda2020sampling,kanwar2020equivariant,%
% toth2019hamiltonian,hoffman2019neutra,albergo2019flow,neklyudov2020involutive,wehenkel2020you,pasarica2010adaptively,dinhRealNVP,wilson74c,%
% tanaka2017towards,schaefer2009investigating,cossu2018testing,rezende2020normalizing}
%
\section{\label{sec:background}Background}
We provide a review of the generic Hamiltonian Monte Carlo (HMC) algorithm and setup some of the relevant notation in
\Secref{subsec:HMC}.
%
\subsection{\label{sec:l2hmc}Generalizing the leapfrog integrator: L2HMC}
%
% \textbf{Notable changes:}
% \begin{enumerate}
%    \item Compared to the original implementation, we carry throughought the updates a discrete index \(k = 0, 1, \ldots,
%       N_{\mathrm{LF}}\), parameterizing the current leapfrog step.%
%
%       In doing so, we are free to consider the case where we use different update functions
%       (\Eqref{eq:new_momentum_update}, \Eqref{eq:new_position_update}), with completely independent step sizes
%       \(\varepsilon^{k}_{j}\), for each of the \(j = x, v\) updates.
%    \item We propose a modified loss function, defined in terms of the topological charge metric,
%       \begin{equation}
%          \mathcal{L}_{\theta}{\left(\xi^{\prime},\xi,A(\xi^{\prime}|\xi)\right)} =
%          \frac{-A(\xi^{\prime}|\xi)\cdot \delta_{\mathcal{Q}_{\mathbb{R}}}(\xi^{\prime}, \xi)}{a^{2}}
%       \end{equation}
%       where \(\delta_{\mathcal{Q}_{\mathbb{R}}}(\xi^{\prime}, \xi) \equiv%
%       {(\mathcal{Q}_{\mathbb{R}}^{\prime} - \mathcal{Q}_{\mathbb{R}})}^{2}\) where \(\mathcal{Q}_{\mathbb{R}}\) is the
%       \emph{topological charge}, defined in \Secref{sec:lattice_gauge_theory}
%    \item We introduce an \emph{annealing schedule} during training so that our target distribution becomes
%       \(p_{t}\propto e^{-\beta_{t}S(x)}\), for \({\{\beta\}}_{t=0}^{N} = \{\beta_{0}, \beta_{1},
%       \ldots, \beta_{N}\}\), where \(t = 0, 1, \ldots, N\) enumerates our training step, \(\beta_{0} < \beta_{1} <
%       \ldots < \beta_{N-1} < \beta_{N} = 1\), and \(\beta_{t+1} - \beta_{t} \ll 1\).
% \end{enumerate}
%

In \citep{levy2017}, the authors propose the L2HMC (``Learning to Hamiltonian Monte Carlo'') algorithm, and demonstrate
its ability to outperform traditional Hamiltonian Monte Carlo (HMC) on a variety of two-dimensional target
distributions.
%
For example, the trained L2HMC sampler is shown to be capable of exploring regions of phase space which are typically
inaccessible with traditional HMC.\@
%
Additionally, they show that the trained sampler is efficient at mixing between modes of a multi-modal target
distribution, a feature which is highly desirable for MCMC simulations of lattice gauge theory.
%


We denote a complete state by \(\xi = (x, v, d)\) with target distribution \(p(\xi) = p(x, v, d) = p(x)\cdot p(v)\cdot
p(d)\).
%
Here we've introduced a binary direction variable (distributed independently of both \(x\) and \(v\))
\(d\sim\mathcal{U}(+,-)\) which is resampled following each Metropolis-Hastings accept/reject step, and can be
interpreted as determining the ``direction'' (forward/backward) of our update.
%
\textbf{The key modification of the L2HMC algorithm is the introduction of six auxiliary functions \(s_{i}, t_{i}, q_{i}\) for \(i
= x, v\) into the leapfrog updates, which are parameterized by weights \(\theta\) in a neural network.}
%
% Before describing the modified leapfrog updates, we first introduce some notation.
%

For simplicity, we consider the forward \(d=+1\) direction, and introduce the notation:
%
\begin{align}
   v^{\prime}_{k} &\equiv \Gamma^{+}_{k}(v_{k};\zeta_{v_{k}})
   = v_{k}\odot \exp{\left(\tfrac{\varepsilon^{k}_{v}}{2}s_{v}^{k}(\zeta_{v_{k}})\right)} -
   \tfrac{\varepsilon^{k}_{v}}{2}{\left[\partial_{x}S(x_{k})\odot\exp{\left(\varepsilon^{k}_{v} q_{v}^{k}(\zeta_{v_{k}})\right)}
      +t_{v}^{k}(\zeta_{v_{k}})\right]},\label{eq:new_momentum_update}\\
   x^{\prime}_{k} &\equiv \Lambda^{+}(x_{k};\zeta_{x_{k}})
   = x_{k}\odot\exp(\varepsilon^{k}_{x} s^{k}_{x}(\zeta_{x_{k}}))
   + \varepsilon^{k}_{x}\left[v^{\prime}_{k}\odot\exp(\varepsilon^{k}_{x} q^{k}_{x}(\zeta_{x_{k}}))
         + t^{k}_{x}(\zeta_{x_{k}})\right]\label{eq:new_position_update}
\end{align}
%
where (1.) \(\zeta_{v_{k}} = (x_{k}, \partial_{x}S(x_{k}), \tau(k))\), \(\zeta_{x_{k}} = (x_{k}, v_{k}, \tau(k))\) are a
shorthand notation for the input passed to the respective \(s, t, q\) network functions\footnote{Note that \(\zeta_{x},
\zeta_{v}\) are chosen to be subsets of the augmented state space that are independent of the variable being updated.}
(2.) \(\tau(k) = {\left[\cos\tfrac{2\pi k}{N_{\mathrm{LF}}}, \sin\tfrac{2\pi k}{N_{\mathrm{LF}}}\right]}\), \(k = 0, 1,
\ldots, N_{\mathrm{LF}}\), is a discrete time variable parameterizing our trajectory, and (3.) we indicate the forward
\(d=+1\) direction by the \(+\) superscript on \(\Gamma^{+}\), and \(\Lambda^{+}\).
%
Using this notation, we can write the complete leapfrog update (in the forward \(d=+1\) direction) as:
%
\begin{enumerate}
   \item Half-step momentum update:%
      \hspace{29pt}\(%
         v^{\prime}_{k} = \Gamma^{+}_{k}(v_{k};\zeta_{v_{k}})%
   \)
   \item Full-step half-position update\footnote{By this we mean we are performing a complete update step that only
      updates half of the components of \(x\) determined by the mask \(\mt\) and its complement \(\mbart\).}:
      \hspace{17pt} \(%
         x^{\prime}_{k} = \mbart\odot x_{k} + \mt\odot \Lambda^{+}_{k}(x_{k};\zeta_{x_{k}})
   \)
   \item Full-step half-position update:%
      \hspace{20pt} \(%
         x^{\prime\prime}_{k} = \mbart\odot\Lambda^{+}_{k}(x^{\prime}_{k};\zeta_{x^{\prime}_{k}}) + \mt\odot x^{\prime}_{k}
   \)
   \item Half-step momentum update:%
      \hspace{24pt} \(%
         v^{\prime\prime}_{k} = \Gamma^{+}(v^{\prime}_{k}; \zeta_{v^{\prime}_{k}})
   \)
\end{enumerate}
%
Note that in order to keep our leapfrog update reversible, we've split the \(x\) update into two sub-updates by
introducing a binary mask \(m^{t} = \mt\odot\mathbbm{1} + \mbart\odot\mathbbm{1}\) that updates half of the components
of \(x\) sequentially, as shown (with the general transformation \(\Lambda^{\pm}\)) in \Figref{fig:splitx}.
%
\begin{figure}[htpb]
   \centering
   \includegraphics[width=\textwidth]{figures/splitx10.pdf}
   \caption{\label{fig:splitx}Illustration of the split \(x\) update. Here we include the general form of the
   transformation \(\Lambda^{\pm}_{k}(x_{k};\zeta_{x_{k}})\).}
\end{figure}
%

As in HMC, we form a complete trajectory by performing \(N_{\mathrm{LF}}\) leapfrog steps in sequence, followed by a
Metropolis-Hastings accept/reject step as described in \Eqref{eq:mhcriteria}.
%
However, unlike in the expression for HMC, we must take into account the overall Jacobian factor from the update
\(\xi\rightarrow\xi^{\prime}\), which can be easily computed as 
%
\begin{equation}
   \left|\tfrac{\partial v^{\prime}_{k}}{\partial v_{k}}\right| 
   = \exp{\left(\tfrac{\varepsilon^{k}_{v}}{2}s^{k}_{v}(\zeta_{v_{k}})\right)},\quad
   \left|\tfrac{\partial x^{\prime}_{k}}{\partial x_{k}}\right| 
   = \exp{\left(\varepsilon^{k}_{x} s^{k}_{x}(\zeta_{x_{k}})\right)}.
\end{equation}
%
So far we've restricted our attention to the forward \((d=+1)\) update, however we can obtain the expressions for the
reverse direction simply by inverting the two functions \(\Gamma^{-}\equiv{\left(\Gamma^{+}\right)}^{-1}\),
\(\Lambda^{-}\equiv{\left(\Lambda^{+}\right)}^{-1}\), and performing the updates in the reverse order.

In order to perform the updates in the generalized leapfrog integrator, we need to evaluate each of the functions
\(s, t, q\).
%
% We maintain separate networks with identical architectures for updating the \(x\) and \(v\) components separately.
%
Without loss of generality\footnote{Because we maintain a separate network with identical architecture for evaluating
the \(s, t, q\) functions in the momentum update \Eqref{eq:new_momentum_update}, the procedure is identical},
we temporarily ignore the discrete leapfrog index \(k\), and restrict our attention to the
\(s_{x}, t_{x}, q_{x}\) functions used in the \(x\) update, \Eqref{eq:new_position_update}.
% \(x_{k} = \Lambda^{+}(x;\zeta_{x})\).
%

Each of the \(s_{x}, t_{x}, q_{x}\) functions takes as input \(\zeta_{x} = (x, v, \tau)\), with \(x\in\mathbb{R}^{n}\),
\(v\in\mathbb{R}^{n}\), and \(\tau \in \mathbb{R}^{2}\).
%
The network splits these inputs and constructs the following intermediate variable (where \(\sigma\) denotes an
arbitrary activation function)
%
\begin{equation}
   z_{1} = \sigma(w_{x}^{T}x + w_{v}^{T}v + w_{\tau}^{T}\tau + b).
\end{equation}
%
This intermediate variable \(z_{1}\) is then passed through another series of fully-connected layers,
%
\begin{equation}
   z_{n} = \sigma(w_{n}^{T} z_{n-1} + b_{n}),\,\, z_{n-1}=\sigma(w_{n-2}^{T}z_{n-2} + b_{n-2}),\,\,%
   \ldots,\,\, z_{2} = \sigma(w_{2}^{T} z_{1} + b_{2}).
\end{equation}
%
The network outputs \(s_{x}, t_{x}, q_{x}\) are then defined in terms of this final hidden variable \(z_{n}\) as
%
\begin{equation}
   s_{x}(\zeta_{x}) = \alpha_{s}\tanh(w_{s}z_{n} + b_{s}),\quad
   t_{x}(\zeta_{x}) = w_{t}^{T}z_{n} + b_{t},\quad
   q_{x}(\zeta_{x}) = \alpha_{q}\tanh(w_{q}z_{n} + b_{q})
\end{equation}
%
where \(\alpha_{s}\), and \(\alpha_{q}\) are trainable scaling factors.
%
The only requirement on the details of the network is that the dimensionality of the outputs \(s_{x}, t_{x}, q_{x}\)
match the dimensionality of our physical variables \(x, v \in\mathbb{R}^{n}\).
%
%%%%%%%%%%%%%%

Next, we introduce a loss function
%
\begin{equation}
   \mathcal{L}_{\theta}{\left(\xi, \xi^{\prime}, A(\xi^{\prime}|\xi)\right)}
      = -\frac{A(\xi^{\prime}|\xi)\cdot \delta(\xi, \xi^{\prime})}{a^{2}}
\end{equation}
%
where \(\delta(\xi, \xi^{\prime})\) is a suitably chosen \emph{metric function}, and \(a\) is a scaling factor.
%
\section{\label{sec:annealing_schedule}Annealing Schedule}
%
To help our sampler overcome the large energy barriers between isolated modes, we introduce an \emph{annealing
schedule}, during the training phase
%
\begin{equation}
   {\{\gamma_{t}\}}_{t=0}^{N} = \{\gamma_{0}, \gamma_{1}, \ldots, \gamma_{N-1}, \gamma_{N}\},
\end{equation}
%
where \(\gamma_{0} < \gamma_{1} < \cdots < \gamma_{N} \equiv 1\), \(\gamma_{t+1} - \gamma_{t} \ll 1\), and \(N\) denotes the
total number of training steps to be performed.
%
Note that we are free to vary \(\gamma\) during the initial training phase as long as we recover the true distribution
with \(\gamma \equiv 1\) at the end of training and evaluate our trained model without this factor.
%
Explicitly, for \(\gamma_{t} < 1\) this rescaling factor helps to reduce the height of the energy barriers, making it
easier for our sampler to explore previously inaccessible regions of the phase space.
%
In terms of this additional annealing schedule, our target distribution picks up an additional index \(t\) to represent
our progress through the training phase, which can be written explicitly as  
%
\begin{equation}
   p_{t}(x)\propto e^{-\gamma_{t} S(x)}, \quad\text{for}\quad t = 0, 1, \ldots, N.
   \label{eq:annealingdistribution}
\end{equation}
%

\section{\label{sec:lattice_gauge_theory}Lattice Gauge Theory}
%
\textbf{Note:} We define the lattice gauge theory in terms of the link variables \(-\pi \leq \varphi_{\mu}(x) < \pi\),
which we explicitly choose to identify as the physical (position) variable \(x\) in \Secref{sec:l2hmc}, i.e.\ 
\begin{equation}
   \varphi_{\mu}(x)\Longleftrightarrow x, \quad p(x)\Longleftrightarrow p(\varphi),\quad \text{etc.}.
\end{equation}
% being the \emph{position} variable \(x\) in \Secref{sec:l2hmc}, i.e.\@ \(\varphi_{\mu}(x) \Longleftrightarrow x\).
%
Our theory is defined on an \(N_{x}\times N_{t}\) two-dimensional lattice with periodic boundary conditions.
%
Let \(U_{\mu}(x) = e^{i\varphi_{\mu}(x)} \in U(1)\), with \(\varphi_{\mu}(x) \in [-\pi,\pi]\) denote the \emph{link
variables}, where \(\varphi_{\mu}(x)\) denotes the link oriented in the \(\hat{\mu}\)-direction located at the site
\(x\).
%
% We denote by \(P\) the \(1\times1\) elementary plaquette with counter-clockwise orientation as shown in
% \Figref{fig:plaquette}.
%
We can write our target distribution \(p_{t}(\varphi)\) in terms of the Wilson action, \(S(\varphi)\) as
%
\begin{equation}
   p_{t}(\varphi) = e^{-\gamma_{t}\beta\cdot S(\varphi)},\quad\text{with}\quad \beta\cdot S(\varphi) = \sum_{P}1 - \cos(\varphi_{P})
   \label{eq:wilsonaction}
\end{equation}
%
and \(\varphi_{P} \equiv \varphi_{\mu}(x) + \varphi_{\nu}(x+\hat{\mu}) - \varphi_{\mu}(x+\hat{\nu})
-\varphi_{\nu}(x)\) is the sum of the link variables around the \(1\times1\) elementary plaquette, as shown in
\Figref{fig:plaquette}.
%
Here, \(\beta = 2 / g_{0}^{2}\) is the inverse coupling constant and \(\beta\rightarrow\infty\) recovers the continuum
limit of the theory. 
%
We consider two physical quantities of interest, namely the \emph{real} and \emph{integer} valued \emph{topological
charge}, (\(\mathcal{Q}_{\mathbb{R}} \in \mathbb{R}\), and \(\mathcal{Q}_{\mathbb{Z}}\in\mathbb{Z}\), respectively)
defined as
%
\begin{equation*}
   \mathcal{Q}_{\mathbb{R}}(\varphi) 
   \equiv \frac{1}{2\pi}\sum_{P}\sin(\varphi_{P}),\quad%
      \mathcal{Q}_{\mathbb{Z}}(\varphi) \equiv \frac{1}{2\pi}\sum_{P}\left\lfloor\varphi_{P}\right\rfloor,
   \quad\text{where}\quad \left\lfloor\varphi_{P}\right\rfloor = \varphi_{P} -
   2\pi\left\lfloor\frac{\varphi_{P}+\pi}{2\pi}\right\rfloor
\end{equation*}
%
\begin{figure}[htpb]
    \centering
    \begin{tikzpicture}
        \draw[->, black, very thick] (0, 0) -- (1, 0) node[anchor=north] {\(\varphi_{\mu}(x)\)};
        \draw[black, very thick] (1, 0) -- (2, 0) {};
        \draw[->, black, very thick] (2, 0) -- (2, 1) node[anchor=west]
            {\(\varphi_{\nu}(x+\hat{\mu})\)};
        \draw[black, very thick] (2, 1) -- (2, 2) {};
        \draw[->, black, very thick] (2, 2) -- (1, 2) node[anchor=south] {\(-\varphi_{\mu}(x+\hat{\nu})\)};
        \draw[black, very thick] (1, 2) -- (0, 2) {};
        \draw[->, black, very thick] (0, 2) -- (0, 1) node[anchor=east] {\(-\varphi_{\nu}(x)\)};
        \draw[black, very thick] (0, 1) -- (0, 0) {};
        \filldraw [black] (0,0) circle (2pt) node[anchor=north east] {\(x\)};
        % \draw[->, gray, very thick] (0.1, 0.1) -- (1.1, 0.1) {\(\mu\)};
        % \draw[->, gray, thick] (0.5, 0.6) -- (1.5, 0.6) {$mu$};
        % \draw[->, gray, thick] (0.6, 0.5) -- (0.6, 1.5) {\(\nu\)};
        \draw[-, black, thick, dashed] (2, 0) -- (3, 0) node[anchor=west] {};
        \draw[-, black, thick, dashed] (0, 0) -- (-1, 0) node[anchor=west] {};
        \draw[-, black, thick, dashed] (0, 0) -- (0, -1) node[anchor=west] {};
        \draw[-, black, thick, dashed] (2, 2) -- (3, 2) node[anchor=west] {};
        \draw[-, black, thick, dashed] (2, 2) -- (2, 3) node[anchor=west] {};
        \draw[-, black, thick, dashed] (0, 2) -- (0, 3) node[anchor=west] {};
        \draw[-, black, thick, dashed] (0, 2) -- (-1, 2) node[anchor=west] {};
        \draw[-, black, thick, dashed] (2, 0) -- (2, -1) node[anchor=west] {};
    \end{tikzpicture}
    \caption{\label{fig:plaquette}Elementary plaquette, \(P\) on the lattice.}
\end{figure}
\begin{equation}
   U_{\mu}(x) = e^{i\varphi_{\mu}(x)}, \quad \varphi_{\mu}(x) \in \left[-\pi, \pi\right]
\end{equation}
%
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{0.48\textwidth}
      \includegraphics[width=\textwidth]{figures/topological_freezing_green.pdf}
      \caption{\label{fig:topological_freezing}Plot of the topological charge history \(\mathcal{Q}_{\mathbb{R}}\) vs MC
      step for both generic HMC (black line), and the trained model (green line)}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.48\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_vs_beta.pdf}
      \caption{\label{fig:autocorr_vs_beta}Plot of the estimated integrated autocorrelation time \(\tau_{\mathrm{int}}^{\mathcal{Q}_{\mathbb{R}}}\)
      vs \(\beta\) for both generic HMC (black dashed line), and the trained model (solid green line).}
   \end{subfigure}
\end{figure}
%

\subsubsection*{Acknowledgments}
This research used resources of the Argonne Leadership Computing Facility (ALCF), which is a DOE office of science user
facility supported under contract DE\_AC02--06CH11357.%
%
This work describes objective technical results and analysis.
%
Any subjective views or opinions that might be expressed in the work do not necessarily represent the views of the u.s.
doe or the united states government.

\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Appendix}
%
\subsection{\label{subsec:HMC}Hamiltonian Monte Carlo}
%
The Hamiltonian Monte Carlo (HMC) algorithm is a widely used technique that allows us to sample from an analytically
known target distribution \(p(x)\) by constructing a chain of states \(\{x^{(0)},
x^{(1)}, \ldots, x^{(n)}\}\), such that \(x^{(n)}\sim p(x)\) in the limit
\(n\rightarrow\infty\).
%
For our purposes, we assume that our target distribution can be expressed as a Boltzmann distribution, \(p(x) =
\tfrac{1}{\mathcal{Z}} e^{-S(x)}\propto e^{-S(x)}\), where \(S(x)\) is the \emph{action} of our
theory, and \(\mathcal{Z}\) is often referred to as the \emph{partition function}, which ensures our target distribution
is correctly normalized to unity.
%
In this case, HMC begins by augmenting the state space with a fictitious momentum variable \(v\), normally
distributed independently of \(x\), i.e.\ \(v\sim\mathcal{N}(0, \mathbbm{1})\).
%
Our joint distribution can then be written as
%
\begin{equation}
   p(x, v) = p(x)\cdot p(v) \propto e^{-S(x)}\cdot e^{-\frac{1}{2}v^{T}v} = e^{-\mathcal{H}(x, v)}
\end{equation}
%
where \(\mathcal{H}(x, v)\) is the Hamiltonian of the joint \((x, v)\) system.
%
Notably, this system obeys Hamilton's equations
%
\begin{equation}
   \dot{x} = \frac{\partial\mathcal{H}}{\partial v},\quad \dot{v} = -\frac{\partial\mathcal{H}}{\partial x}
\end{equation}
%
which can be integrated using the \emph{leapfrog integrator} along iso-probability contours defined by \(\mathcal{H} =
\text{const}\).
%
Explicitly, for a step size \(\varepsilon\) and initial state \(\xi = (x, v)\), the leapfrog integrator generates a
proposal configuration \(\xi^{\prime} \equiv (x^{\prime}, v^{\prime})\) by performing the following series of updates: 
%
\begin{enumerate}
   \item Half-step momentum update: \hspace{12pt}\(%
      v^{1/2} \equiv v{\left(t+\frac{\varepsilon}{2}\right)} = v-\frac{\varepsilon}{2}\partial_{x}S(x)
   \)
      %
      % \begin{equation}
      %    v^{1/2} \equiv v{\left(t+\frac{\varepsilon}{2}\right)} = v-\frac{\varepsilon}{2}\partial_{x}S(x)
      %    \label{eq:original_momentum_update}
      % \end{equation}
      %
   \item Full-step position update: \hspace{36pt}\(%
      x^{\prime} \equiv x(t+\varepsilon) = x + \varepsilon v^{1/2}
   \)
      %
      % \begin{equation}
      %    x^{\prime} \equiv x(t+\varepsilon) = x + \varepsilon v^{1/2}
      %    \label{eq:original_position_update}
      % \end{equation}
      %
   \item Half-step momentum update:
      \hspace{18pt} \(%
         v^{\prime} \equiv v(t+\varepsilon) = v^{1/2} - \frac{\varepsilon}{2}\partial_{x} S(x^{\prime})
   \)
      %
      % \begin{equation}
      %    v^{\prime} \equiv v(t+\varepsilon) = v^{1/2} - \frac{\varepsilon}{2}\partial_{x} S(x^{\prime})
      % \end{equation}
      %
\end{enumerate}
%
We can then construct a complete \emph{trajectory} of length \(\lambda = \varepsilon\cdot N_{\mathrm{LF}}\) by
performing \(N_{\mathrm{LF}}\) leapfrog steps in sequence.
%
At the end of our trajectory, we either accept or reject the proposal configuration according to the Metropolis-Hastings
acceptance criteria,
%
\begin{equation}
   x_{i+1} =
   \begin{cases}%
      x^{\prime} &\mbox{with probability } A(\xi^{\prime}|\xi) \\
      x &\mbox{with probability } (1 - A(\xi^{\prime}|\xi)), \quad\text{where}\quad %
         A(\xi^{\prime}|\xi) = \min\left\{%
            1, \frac{p(\xi^{\prime})}{p(\xi)}\left|\frac{\partial{\xi^{\prime}}}{\partial\xi^{T}}\right|%
         \right\}.
   \end{cases}
   \label{eq:mhcriteria}
\end{equation}
%
% where
%
The generic leapfrog integrator is known to be symplectic (conserves energy), so the Jacobian factor reduces to
\(\left|\frac{\partial\xi^{\prime}}{\partial\xi^{T}}\right| = 1\). 
%

\subsection{\label{subsec:extra_figures}Extra Figures}
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{0.31\textwidth}
      % \includegraphics[width=\textwidth]{figures/plaqsf.pdf}
      \includegraphics[width=\textwidth]{figures/2021-03-09/plaqsf_1758.pdf}
      \caption{\label{fig:plaqsf}Error in the average plaquette, \(\langle\varphi_{P}-\varphi_{P}^{*}\rangle\) vs
      leapfrog step.}
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.31\textwidth}
      % \includegraphics[width=\textwidth]{figures/sinQf.pdf}
      \includegraphics[width=\textwidth]{figures/2021-03-09/sinQf_1755.pdf}
      % \includegraphics[width=\textwidth]{figures/2021-03-09/sinQf_1754.pdf}
      \caption{\label{fig:sinQf}The real-valued topological charge, \(\mathcal{Q}_{\mathbb{R}}\) vs leapfrog step.}%
   \end{subfigure}
   \hfill
   \begin{subfigure}{0.31\textwidth}
      % \includegraphics[width=\textwidth]{figures/hwf.pdf}
      \includegraphics[width=\textwidth]{figures/2021-03-09/hwf_1756.pdf}
      \caption{\label{fig:hwf}The rescaled energy, \(\mathcal{H}-\sum\log\|\mathcal{J}\|\) vs leapfrog step.}
         %\(\mathcal{H} - \left|\frac{\partial\xi^{\prime}}{\partial\xi^{T}}\right|\) 
   \end{subfigure}
   \hfill
   \caption{\label{fig:transformations}Illustration of how various observables vary over a single trajectory for the
   trained sampler.}
\end{figure}
%
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{\linewidth}
      \includegraphics[width=\linewidth]{figures/autocorr_vs_mc_step.pdf}
      \caption{Plot of the integrated autocorrelation time \(\tau_{\mathrm{int}}^{\mathcal{Q}_{\mathbb{R}}}\) vs
      MC step for both generic HMC and the trained model, for \(\beta = 2, 3, \ldots, 7\) arranged from left to right.}
   % \end{subfigure}
   % \begin{subfigure}{\textwidth}
      \includegraphics[width=\linewidth]{figures/autocorr_vs_traj_len.pdf}
      \caption{Plot of the integrated autocorrelation time \(\tau_{\mathrm{int}}^{\mathcal{Q}_{\mathbb{R}}}\) vs
      \(\lambda\) for both generic HMC and the trained model.}
   \end{subfigure}
\end{figure}
%

\subsection{\label{subsec:autocorr_plots}Autocorrelation Plots}
%
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2153_b2.pdf}
   \end{subfigure}
   % \hfill
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2151_b3.pdf}
   \end{subfigure}
   \caption{\label{fig:autocorrbeta23}Plot of the estimated integrated autocorrelation time of the topological charge,
      \(\tau_{\mathrm{int}}^{\mathcal{Q}}\) vs MC Step (left), and trajectory length \(\lambda\) (right) for both HMC
   (grey circles) and the trained model (pink squares)}
\end{figure}
%
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2152_b4.pdf}
   \end{subfigure}
   % \hfill
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2152_b5.pdf}
   \end{subfigure}
   \caption{\label{fig:autocorrbeta45}Plot of the estimated integrated autocorrelation time of the topological charge,
      \(\tau_{\mathrm{int}}^{\mathcal{Q}}\) vs MC Step (left), and trajectory length \(\lambda\) (right) for both HMC
   (grey circles) and the trained model (pink squares)}
\end{figure}
%
\begin{figure}[htpb]
   \centering
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2152_b6.pdf}
   \end{subfigure}
   % \hfill
   \begin{subfigure}{0.4\textwidth}
      \includegraphics[width=\textwidth]{figures/autocorr_plots_2021_03_09/autocorr_vs_traj_len_2152_b7.pdf}
   \end{subfigure}
   \caption{\label{fig:autocorrbeta67}Plot of the estimated integrated autocorrelation time of the topological charge,
      \(\tau_{\mathrm{int}}^{\mathcal{Q}}\) vs MC Step (left), and trajectory length \(\lambda\) (right) for both HMC
   (grey circles) and the trained model (pink squares)}
\end{figure}
%
%
\subsection{\label{subsec:algorithm}Training Algorithm}
%
\begin{algorithm}[htpb]%
   % \SetAlgoSkip{bigskip}%
   % \SetVlineSkip{20pt}
   \SetNlSty{texttt}{[}{]}
   % \SetAlgoRefName{training}
   \SetAlgoLined%
   \SetAlgoVlined%
   \SetKwProg{Fn}{def}{\string:}{}%
   \SetKwFunction{Range}{range}%
   \SetKwFor{For}{\color{blue}\textbf{\texttt{for}}}{\string:}{}\color{black}%
   \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
   \SetKwFor{While}{while}{:}{fintq}%
   \SetKwInOut{Input}{\color{red}{\textbf{\texttt{input}}}\color{black}}%
   \SetKwInOut{Output}{\color{red}{\textbf{\texttt{output}}}\color{black}}%
   \AlgoDontDisplayBlockMarkers\SetAlgoNoEnd%
   \DontPrintSemicolon%
   \caption{\label{alg:training_algorithm}Training procedure}%
   \Input{%
      % {}\\
      \begin{enumerate}
         \item \texttt{Target distribution,} \(p_{t}(x)\propto e^{-\beta_{t} U(x)}\)
         \item \texttt{Loss function}, \(\mathcal{L}_{\theta}(\xi^{\prime},\xi,
            A(\xi^{\prime}|\xi))\)
         \item \texttt{Learning rate schedule}, \({\{\alpha_{t}\}}_{t=0}^{N_{\mathrm{train}}}\)
         \item \texttt{Annealing schedule}, \({\{\beta_{t}\}}_{t=0}^{N_{\mathrm{train}}}\)
         \item \texttt{Batch of initial states}, \(x\)
      \end{enumerate}
   }%
   \texttt{Initialize weights} \(\theta\)\;
   % {}\;
   \For{\(0 \leq t < N_{\mathrm{train}}\)}{%
      % {}\;
      \texttt{update:} \(p_{t}(x)\propto e^{-\beta_{t}U(x)}\)\;
      \texttt{resample:} \(v\sim\mathcal{N}(0, \mathbbm{1})\)\;
      \texttt{resample:} \(d\sim\mathcal{U}(+,-)\)\;
      \texttt{construct:} \(\xi \equiv (x, v, d)\)\;
      % {}\;
      \For{\(0 \leq \ell < N_{\mathrm{LF}}\)}{%
         \texttt{propose:} \(\xi_{\ell}^{\prime}\leftarrow\mathbf{FL}_{\ell}^{\pm}\xi_{\ell}\)
      }
      \texttt{compute:} \(A(\xi^{\prime}|\xi) =%
      \min\left\{1,\frac{p(\xi^{\prime})}{p(\xi)}\left|\frac{\partial%
      \xi^{\prime}}{\partial \xi^{T}}\right|\right\}\)\;
      % {}\;
      \texttt{update:} \(\mathcal{L}\leftarrow \mathcal{L}_{\theta}(\xi^{\prime},\xi,%
      A(\xi^{\prime}|\xi))\)\;
      % {}\;
      \texttt{backprop:} \(\theta\ \leftarrow\ \theta-\alpha_t \nabla_{\theta} \mathcal{L}\)\;
      % {}\;
      \texttt{assign:} \(x_{t+1} \leftarrow
      \begin{cases}
         x^{\prime} &\mbox{with probability } A(\xi^{\prime}|\xi) \\
         x &\mbox{with probability } (1 - A(\xi^{\prime}|\xi)).%
      \end{cases}%
      \)\;
   }\;
   % \caption{\label{alg}Training algorithm}
\end{algorithm}
\end{document}
